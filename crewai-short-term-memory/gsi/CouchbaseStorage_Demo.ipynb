{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrewAI with Couchbase Short-Term Memory using GSI Vector Search\n",
    "\n",
    "This notebook demonstrates how to implement a custom storage backend for CrewAI's memory system using Couchbase and GSI (Global Secondary Index) vector search. GSI vector search provides high-performance semantic search capabilities and is available in Couchbase Server 8.0.0+. Alternatively if you want to perform semantic search using the FTS index, please take a look at [this.](../fts/CouchbaseStorage_Demo.ipynb)\n",
    "\n",
    "Here's a breakdown of each section:\n",
    "\n",
    "How to run this tutorial\n",
    "----------------------\n",
    "This tutorial is available as a Jupyter Notebook (.ipynb file) that you can run \n",
    "interactively. You can access the original notebook here.\n",
    "\n",
    "You can either:\n",
    "- Download the notebook file and run it on [Google Colab](https://colab.research.google.com)\n",
    "- Run it on your system by setting up the Python environment\n",
    "\n",
    "Before you start\n",
    "---------------\n",
    "\n",
    "**Important**: GSI vector search requires Couchbase Server 8.0.0+ or Couchbase Capella with Query Service enabled.\n",
    "\n",
    "1. Create and Deploy Your Free Tier Operational cluster on [Capella](https://cloud.couchbase.com/sign-up)\n",
    "   - To get started with [Couchbase Capella](https://cloud.couchbase.com), create an account and use it to deploy \n",
    "     a forever free tier operational cluster\n",
    "   - This account provides you with an environment where you can explore and learn \n",
    "     about Capella with no time constraint\n",
    "   - To learn more, please follow the [Getting Started Guide](https://docs.couchbase.com/cloud/get-started/create-account.html)\n",
    "\n",
    "2. Couchbase Capella Configuration\n",
    "   When running Couchbase using Capella, the following prerequisites need to be met:\n",
    "   - Create the database credentials to access the required bucket (Read and Write) \n",
    "     used in the application\n",
    "   - Allow access to the Cluster from the IP on which the application is running by following the [Network Security documentation](https://docs.couchbase.com/cloud/security/security.html#public-access)\n",
    "   - Ensure Query Service is enabled on your cluster for GSI vector search functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory in AI Agents\n",
    "\n",
    "Memory in AI agents is a crucial capability that allows them to retain and utilize information across interactions, making them more effective and contextually aware. Without memory, agents would be limited to processing only the immediate input, lacking the ability to build upon past experiences or maintain continuity in conversations.\n",
    "\n",
    "> Note: This section on memory types and functionality is adapted from the CrewAI documentation.\n",
    "\n",
    "## Types of Memory in AI Agents\n",
    "\n",
    "### Short-term Memory\n",
    "- Retains recent interactions and context\n",
    "- Typically spans the current conversation or session  \n",
    "- Helps maintain coherence within a single interaction flow\n",
    "- In CrewAI, this is what we're implementing with the Couchbase storage\n",
    "\n",
    "### Long-term Memory\n",
    "- Stores persistent knowledge across multiple sessions\n",
    "- Enables agents to recall past interactions even after long periods\n",
    "- Helps build cumulative knowledge about users, preferences, and past decisions\n",
    "- While this implementation is labeled as \"short-term memory\", the Couchbase storage backend can be effectively used for long-term memory as well, thanks to Couchbase's persistent storage capabilities and enterprise-grade durability features\n",
    "\n",
    "\n",
    "\n",
    "## How Memory Works in Agents\n",
    "Memory in AI agents typically involves:\n",
    "- Storage: Information is encoded and stored in a database (like Couchbase, ChromaDB, or other vector stores)\n",
    "- Retrieval: Relevant memories are fetched based on semantic similarity to current context\n",
    "- Integration: Retrieved memories are incorporated into the agent's reasoning process\n",
    "\n",
    "In the CrewAI example, the CouchbaseStorage class implements:\n",
    "- save(): Stores new memories with metadata\n",
    "- search(): Retrieves relevant memories based on semantic similarity\n",
    "- reset(): Clears stored memories when needed\n",
    "\n",
    "## Benefits of Memory in AI Agents\n",
    "- Contextual Understanding: Agents can refer to previous parts of a conversation\n",
    "- Personalization: Remembering user preferences and past interactions\n",
    "- Learning and Adaptation: Building knowledge over time to improve responses\n",
    "- Task Continuity: Resuming complex tasks across multiple interactions\n",
    "- Collaboration: In multi-agent systems like CrewAI, memory enables agents to build on each other's work\n",
    "\n",
    "## Memory in CrewAI Specifically\n",
    "In CrewAI, memory serves several important functions:\n",
    "- Agent Specialization: Each agent can maintain its own memory relevant to its expertise\n",
    "- Knowledge Transfer: Agents can share insights through memory when collaborating on tasks\n",
    "- Process Continuity: In sequential processes, later agents can access the work of earlier agents\n",
    "- Contextual Awareness: Agents can reference previous findings when making decisions\n",
    "\n",
    "The vector-based approach (using embeddings) is particularly powerful because it allows for semantic search - finding memories that are conceptually related to the current context, not just exact keyword matches.\n",
    "\n",
    "By implementing custom storage like Couchbase, you gain additional benefits like persistence, scalability, and the ability to leverage enterprise-grade database features for your agent memory systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "\n",
    "This section installs the necessary Python packages:\n",
    "- `crewai`: The main CrewAI framework\n",
    "- `langchain-couchbase`: LangChain integration for Couchbase\n",
    "- `langchain-openai`: LangChain integration for OpenAI\n",
    "- `python-dotenv`: For loading environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet crewai==0.186.1 langchain-couchbase==0.5.0rc1 langchain-openai==0.3.33 python-dotenv==1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "import os\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "from dotenv import load_dotenv\n",
    "from crewai.memory.storage.rag_storage import RAGStorage\n",
    "from crewai.memory.short_term.short_term_memory import ShortTermMemory\n",
    "from crewai import Agent, Crew, Task, Process\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.options import ClusterOptions\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.diagnostics import PingState, ServiceType\n",
    "from langchain_couchbase.vectorstores import CouchbaseQueryVectorStore\n",
    "from langchain_couchbase.vectorstores import DistanceStrategy\n",
    "from langchain_couchbase.vectorstores import IndexType\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# Configure logging (disabled)\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Sensitive Information\n",
    "\n",
    "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like database credentials, and specific configuration names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script uses environment variables to store sensitive information, enhancing the overall security and maintainability of your code by avoiding hardcoded values.\n",
    "\n",
    "### Setting Up Environment Variables\n",
    "\n",
    "> **Note:** This implementation reads configuration parameters from environment variables. Before running this notebook, you need to set the following environment variables:\n",
    ">\n",
    "> - `OPENAI_API_KEY`: Your OpenAI API key for generating embeddings\n",
    "> - `CB_HOST`: Couchbase cluster connection string (e.g., \"couchbases://cb.example.com\")\n",
    "> - `CB_USERNAME`: Username for Couchbase authentication\n",
    "> - `CB_PASSWORD`: Password for Couchbase authentication\n",
    "> - `CB_BUCKET_NAME` (optional): Bucket name (defaults to \"vector-search-testing\")\n",
    "> - `SCOPE_NAME` (optional): Scope name (defaults to \"shared\")\n",
    "> - `COLLECTION_NAME` (optional): Collection name (defaults to \"crew\")\n",
    "> - `INDEX_NAME` (optional): GSI vector index name (defaults to \"vector_search_crew\")\n",
    ">\n",
    "> You can set these variables in a `.env` file in the same directory as this notebook, or set them directly in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"./.env\")\n",
    "\n",
    "# Verify environment variables\n",
    "required_vars = ['OPENAI_API_KEY', 'CB_HOST', 'CB_USERNAME', 'CB_PASSWORD']\n",
    "for var in required_vars:\n",
    "    if not os.getenv(var):\n",
    "        raise ValueError(f\"{var} environment variable is required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding GSI Vector Search Types\n",
    "\n",
    "Couchbase offers two types of GSI vector indexes for different use cases:\n",
    "\n",
    "Hyperscale Vector Indexes (BHIVE)\n",
    "- Best for pure vector searches - content discovery, recommendations, semantic search\n",
    "- High performance with low memory footprint - designed to scale to billions of vectors\n",
    "- Optimized for concurrent operations - supports simultaneous searches and inserts\n",
    "- Use when: You primarily perform vector-only queries without complex scalar filtering\n",
    "- Ideal for: Large-scale semantic search, recommendation systems, content discovery\n",
    "\n",
    "Composite Vector Indexes \n",
    "- Best for filtered vector searches - combines vector search with scalar value filtering\n",
    "- Efficient pre-filtering - scalar attributes reduce the vector comparison scope\n",
    "- Use when: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n",
    "- Ideal for: Compliance-based filtering, user-specific searches, time-bounded queries\n",
    "\n",
    "Choosing the Right Index Type\n",
    "- Start with Hyperscale Vector Index for pure vector searches and large datasets\n",
    "- Use Composite Vector Index when scalar filters significantly reduce your search space\n",
    "- Consider your dataset size: Hyperscale scales to billions, Composite works well for tens of millions to billions\n",
    "\n",
    "For this CrewAI memory implementation, we'll use **BHIVE** as it's optimized for pure semantic search scenarios typical in AI agent memory systems.\n",
    "\n",
    "## Understanding Index Configuration (Couchbase 8.0 Feature)\n",
    "\n",
    "The index_description parameter controls how Couchbase optimizes vector storage and search performance through centroids and quantization:\n",
    "\n",
    "Format: `'IVF[<centroids>],{PQ|SQ}<settings>'`\n",
    "\n",
    "Centroids (IVF - Inverted File):\n",
    "- Controls how the dataset is subdivided for faster searches\n",
    "- More centroids = faster search, slower training  \n",
    "- Fewer centroids = slower search, faster training\n",
    "- If omitted (like IVF,SQ8), Couchbase auto-selects based on dataset size\n",
    "\n",
    "Quantization Options:\n",
    "- SQ (Scalar Quantization): SQ4, SQ6, SQ8 (4, 6, or 8 bits per dimension)\n",
    "- PQ (Product Quantization): PQ<subquantizers>x<bits> (e.g., PQ32x8)\n",
    "- Higher values = better accuracy, larger index size\n",
    "\n",
    "Common Examples:\n",
    "- IVF,SQ8 - Auto centroids, 8-bit scalar quantization (good default)\n",
    "- IVF1000,SQ6 - 1000 centroids, 6-bit scalar quantization  \n",
    "- IVF,PQ32x8 - Auto centroids, 32 subquantizers with 8 bits\n",
    "\n",
    "For detailed configuration options, see the [Quantization & Centroid Settings](https://preview.docs-test.couchbase.com/docs-server-DOC-12565_vector_search_concepts/server/current/vector-index/hyperscale-vector-index.html#algo_settings).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement CouchbaseStorage with GSI Vector Search\n",
    "\n",
    "This section demonstrates the implementation of a custom vector storage solution using Couchbase GSI vector search:\n",
    "\n",
    "> **Note on Implementation:** This example uses the LangChain Couchbase integration (`langchain_couchbase`) with `CouchbaseQueryVectorStore` for GSI-based vector search. This provides high-performance semantic search capabilities using Couchbase's Query Service and GSI vector indexes.\n",
    "\n",
    "> **GSI Requirements:** GSI vector search requires Couchbase Server 8.0.0+ or Couchbase Capella with Query Service enabled.\n",
    "\n",
    "> For more information on GSI vector search, refer to:\n",
    "> - [Couchbase GSI Vector Search Documentation](https://docs.couchbase.com/server/current/vector-index/use-vector-indexes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchbaseStorage(RAGStorage):\n",
    "    \"\"\"\n",
    "    Extends RAGStorage to handle embeddings for memory entries using Couchbase GSI Vector Search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, type: str, allow_reset: bool = True, embedder_config: Optional[Dict[str, Any]] = None, crew: Optional[Any] = None):\n",
    "        \"\"\"Initialize CouchbaseStorage with GSI vector search configuration.\"\"\"\n",
    "        super().__init__(type, allow_reset, embedder_config, crew)\n",
    "        self._initialize_app()\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        limit: int = 3,\n",
    "        filter: Optional[dict] = None,\n",
    "        score_threshold: float = 0,\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search memory entries using GSI vector similarity.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Add type filter\n",
    "            search_filter = {\"memory_type\": self.type}\n",
    "            if filter:\n",
    "                search_filter.update(filter)\n",
    "\n",
    "            # Execute search using GSI vector search\n",
    "            results = self.vector_store.similarity_search_with_score(\n",
    "                query,\n",
    "                k=limit,\n",
    "                filter=search_filter\n",
    "            )\n",
    "            \n",
    "            # Format results and deduplicate by content\n",
    "            seen_contents = set()\n",
    "            formatted_results = []\n",
    "            \n",
    "            for i, (doc, score) in enumerate(results):\n",
    "                # Note: In GSI vector search, lower scores indicate higher similarity\n",
    "                if score <= (1.0 - score_threshold):  # Convert threshold for GSI distance metric\n",
    "                    content = doc.page_content\n",
    "                    if content not in seen_contents:\n",
    "                        seen_contents.add(content)\n",
    "                        formatted_results.append({\n",
    "                            \"id\": doc.metadata.get(\"memory_id\", str(i)),\n",
    "                            \"metadata\": doc.metadata,\n",
    "                            \"context\": content,\n",
    "                            \"score\": float(score)\n",
    "                        })\n",
    "            \n",
    "            logger.info(f\"Found {len(formatted_results)} unique results for query: {query}\")\n",
    "            return formatted_results\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Search failed: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def save(self, value: Any, metadata: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Save a memory entry with metadata.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate unique ID\n",
    "            memory_id = str(uuid.uuid4())\n",
    "            timestamp = int(time.time() * 1000)\n",
    "            \n",
    "            # Prepare metadata (create a copy to avoid modifying references)\n",
    "            if not metadata:\n",
    "                metadata = {}\n",
    "            else:\n",
    "                metadata = metadata.copy()  # Create a copy to avoid modifying references\n",
    "                \n",
    "            # Process agent-specific information if present\n",
    "            agent_name = metadata.get('agent', 'unknown')\n",
    "                \n",
    "            # Clean up value if it has the typical LLM response format\n",
    "            value_str = str(value)\n",
    "            if \"Final Answer:\" in value_str:\n",
    "                # Extract just the actual content - everything after \"Final Answer:\"\n",
    "                parts = value_str.split(\"Final Answer:\", 1)\n",
    "                if len(parts) > 1:\n",
    "                    value = parts[1].strip()\n",
    "                    logger.info(f\"Cleaned up response format for agent: {agent_name}\")\n",
    "            elif value_str.startswith(\"Thought:\"):\n",
    "                # Handle thought/final answer format\n",
    "                if \"Final Answer:\" in value_str:\n",
    "                    parts = value_str.split(\"Final Answer:\", 1)\n",
    "                    if len(parts) > 1:\n",
    "                        value = parts[1].strip()\n",
    "                        logger.info(f\"Cleaned up thought process format for agent: {agent_name}\")\n",
    "            \n",
    "            # Update metadata\n",
    "            metadata.update({\n",
    "                \"memory_id\": memory_id,\n",
    "                \"memory_type\": self.type,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"source\": \"crewai\"\n",
    "            })\n",
    "\n",
    "            # Log memory information for debugging\n",
    "            value_preview = str(value)[:100] + \"...\" if len(str(value)) > 100 else str(value)\n",
    "            metadata_preview = {k: v for k, v in metadata.items() if k != \"embedding\"}\n",
    "            logger.info(f\"Saving memory for Agent: {agent_name}\")\n",
    "            logger.info(f\"Memory value preview: {value_preview}\")\n",
    "            logger.info(f\"Memory metadata: {metadata_preview}\")\n",
    "            \n",
    "            # Convert value to string if needed\n",
    "            if isinstance(value, (dict, list)):\n",
    "                value = json.dumps(value)\n",
    "            elif not isinstance(value, str):\n",
    "                value = str(value)\n",
    "\n",
    "            # Save to GSI vector store\n",
    "            self.vector_store.add_texts(\n",
    "                texts=[value],\n",
    "                metadatas=[metadata],\n",
    "                ids=[memory_id]\n",
    "            )\n",
    "            logger.info(f\"Saved memory {memory_id}: {value[:100]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Save failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the memory storage if allowed.\"\"\"\n",
    "        if not self.allow_reset:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Delete documents of this memory type\n",
    "            self.cluster.query(\n",
    "                f\"DELETE FROM `{self.bucket_name}`.`{self.scope_name}`.`{self.collection_name}` WHERE memory_type = $type\",\n",
    "                type=self.type\n",
    "            ).execute()\n",
    "            logger.info(f\"Reset memory type: {self.type}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Reset failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_app(self):\n",
    "        \"\"\"Initialize Couchbase connection and GSI vector store.\"\"\"\n",
    "        try:\n",
    "            # Initialize embeddings\n",
    "            if self.embedder_config and self.embedder_config.get(\"provider\") == \"openai\":\n",
    "                self.embeddings = OpenAIEmbeddings(\n",
    "                    openai_api_key=os.getenv('OPENAI_API_KEY'),\n",
    "                    model=self.embedder_config.get(\"config\", {}).get(\"model\", \"text-embedding-3-small\")\n",
    "                )\n",
    "            else:\n",
    "                self.embeddings = OpenAIEmbeddings(\n",
    "                    openai_api_key=os.getenv('OPENAI_API_KEY'),\n",
    "                    model=\"text-embedding-3-small\"\n",
    "                )\n",
    "\n",
    "            # Connect to Couchbase\n",
    "            auth = PasswordAuthenticator(\n",
    "                os.getenv('CB_USERNAME', ''),\n",
    "                os.getenv('CB_PASSWORD', '')\n",
    "            )\n",
    "            options = ClusterOptions(auth)\n",
    "            \n",
    "            # Initialize cluster connection\n",
    "            self.cluster = Cluster(os.getenv('CB_HOST', ''), options)\n",
    "            self.cluster.wait_until_ready(timedelta(seconds=5))\n",
    "\n",
    "            # Check Query service (required for GSI vector search)\n",
    "            ping_result = self.cluster.ping()\n",
    "            query_available = False\n",
    "            for service_type, endpoints in ping_result.endpoints.items():\n",
    "                if service_type.name == 'Query':  # Query Service for GSI\n",
    "                    for endpoint in endpoints:\n",
    "                        if endpoint.state == PingState.OK:\n",
    "                            query_available = True\n",
    "                            logger.info(f\"Query service is responding at: {endpoint.remote}\")\n",
    "                            break\n",
    "                    break\n",
    "            if not query_available:\n",
    "                raise RuntimeError(\"Query service not found or not responding. GSI vector search requires Query Service.\")\n",
    "            \n",
    "            # Set up storage configuration\n",
    "            self.bucket_name = os.getenv('CB_BUCKET_NAME', 'vector-search-testing')\n",
    "            self.scope_name = os.getenv('SCOPE_NAME', 'shared')\n",
    "            self.collection_name = os.getenv('COLLECTION_NAME', 'crew')\n",
    "            self.index_name = os.getenv('INDEX_NAME', 'vector_search_crew_gsi')\n",
    "\n",
    "            # Initialize GSI vector store\n",
    "            self.vector_store = CouchbaseQueryVectorStore(\n",
    "                cluster=self.cluster,\n",
    "                bucket_name=self.bucket_name,\n",
    "                scope_name=self.scope_name,\n",
    "                collection_name=self.collection_name,\n",
    "                embedding=self.embeddings,\n",
    "                distance_metric=DistanceStrategy.COSINE,\n",
    "            )\n",
    "            logger.info(f\"Initialized CouchbaseStorage with GSI vector search for type: {self.type}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Initialization failed: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Basic Storage\n",
    "\n",
    "Test storing and retrieving a simple memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage\n",
    "storage = CouchbaseStorage(\n",
    "    type=\"short_term\",\n",
    "    embedder_config={\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\"model\": \"text-embedding-3-small\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Reset storage\n",
    "storage.reset()\n",
    "\n",
    "# Test storage\n",
    "test_memory = \"Pep Guardiola praised Manchester City's current form, saying 'The team is playing well, we are in a good moment. The way we are training, the way we are playing - I am really pleased.'\"\n",
    "test_metadata = {\"category\": \"sports\", \"test\": \"initial_memory\"}\n",
    "storage.save(test_memory, test_metadata)\n",
    "\n",
    "# Test search\n",
    "results = storage.search(\"What did Guardiola say about Manchester City?\", limit=1)\n",
    "for result in results:\n",
    "    print(f\"Found: {result['context']}\\nScore: {result['score']}\\nMetadata: {result['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GSI Vector Index\n",
    "\n",
    "Before we can use the GSI vector store for searching, we need to create a GSI vector index. This step is crucial for GSI vector search to work properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GSI BHIVE vector index for optimal performance\n",
    "try:\n",
    "    storage.vector_store.create_index(\n",
    "        index_type=IndexType.BHIVE,\n",
    "        index_name=storage.index_name,\n",
    "        index_description=\"IVF,SQ8\"  # Auto-selected centroids with 8-bit scalar quantization\n",
    "    )\n",
    "    print(f\"Created GSI BHIVE vector index: {storage.index_name}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"GSI vector index '{storage.index_name}' already exists, continuing...\")\n",
    "    else:\n",
    "        print(f\"Error creating GSI index: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CrewAI Integration\n",
    "\n",
    "Create agents and tasks to test memory retention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ShortTermMemory with our storage\n",
    "memory = ShortTermMemory(storage=storage)\n",
    "\n",
    "# Initialize language model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Create agents with memory\n",
    "sports_analyst = Agent(\n",
    "    role='Sports Analyst',\n",
    "    goal='Analyze Manchester City performance',\n",
    "    backstory='Expert at analyzing football teams and providing insights on their performance',\n",
    "    llm=llm,\n",
    "    memory=True,\n",
    "    memory_storage=memory\n",
    ")\n",
    "\n",
    "journalist = Agent(\n",
    "    role='Sports Journalist',\n",
    "    goal='Create engaging football articles',\n",
    "    backstory='Experienced sports journalist who specializes in Premier League coverage',\n",
    "    llm=llm,\n",
    "    memory=True,\n",
    "    memory_storage=memory\n",
    ")\n",
    "\n",
    "# Create tasks\n",
    "analysis_task = Task(\n",
    "    description='Analyze Manchester City\\'s recent performance based on Pep Guardiola\\'s comments: \"The team is playing well, we are in a good moment. The way we are training, the way we are playing - I am really pleased.\"',\n",
    "    agent=sports_analyst,\n",
    "    expected_output=\"A comprehensive analysis of Manchester City's current form based on Guardiola's comments.\"\n",
    ")\n",
    "\n",
    "writing_task = Task(\n",
    "    description='Write a sports article about Manchester City\\'s form using the analysis and Guardiola\\'s comments.',\n",
    "    agent=journalist,\n",
    "    context=[analysis_task],\n",
    "    expected_output=\"An engaging sports article about Manchester City's current form and Guardiola's perspective.\"\n",
    ")\n",
    "\n",
    "# Create crew with memory\n",
    "crew = Crew(\n",
    "    agents=[sports_analyst, journalist],\n",
    "    tasks=[analysis_task, writing_task],\n",
    "    process=Process.sequential,\n",
    "    memory=True,\n",
    "    short_term_memory=memory,  # Explicitly pass our memory implementation\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the crew\n",
    "result = crew.kickoff()\n",
    "\n",
    "print(\"\\nCrew Result:\")\n",
    "print(\"-\" * 80)\n",
    "print(result)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Memory Retention\n",
    "\n",
    "Query the stored memories to verify retention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for memories to be stored\n",
    "time.sleep(2)\n",
    "\n",
    "# List all documents in the collection\n",
    "try:\n",
    "    # Query to fetch all documents of this memory type\n",
    "    query_str = f\"SELECT META().id, * FROM `{storage.bucket_name}`.`{storage.scope_name}`.`{storage.collection_name}` WHERE memory_type = $type\"\n",
    "    query_result = storage.cluster.query(query_str, type=storage.type)\n",
    "    \n",
    "    print(f\"\\nAll memory entries in Couchbase:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, row in enumerate(query_result, 1):\n",
    "        doc_id = row.get('id')\n",
    "        memory_id = row.get(storage.collection_name, {}).get('memory_id', 'unknown')\n",
    "        content = row.get(storage.collection_name, {}).get('text', '')[:100] + \"...\"  # Truncate for readability\n",
    "        \n",
    "        print(f\"Entry {i}:\")\n",
    "        print(f\"ID: {doc_id}\")\n",
    "        print(f\"Memory ID: {memory_id}\")\n",
    "        print(f\"Content: {content}\")\n",
    "        print(\"-\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to list memory entries: {str(e)}\")\n",
    "\n",
    "# Test memory retention\n",
    "memory_query = \"What is Manchester City's current form according to Guardiola?\"\n",
    "memory_results = storage.search(\n",
    "    query=memory_query,\n",
    "    limit=5,  # Increased to see more results\n",
    "    score_threshold=0.0  # Lower threshold to see all results\n",
    ")\n",
    "\n",
    "print(\"\\nMemory Search Results:\")\n",
    "print(\"-\" * 80)\n",
    "for result in memory_results:\n",
    "    print(f\"Context: {result['context']}\")\n",
    "    print(f\"Score: {result['score']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Try a more specific query to find agent interactions\n",
    "interaction_query = \"Manchester City playing style analysis tactical\"\n",
    "interaction_results = storage.search(\n",
    "    query=interaction_query,\n",
    "    limit=5,\n",
    "    score_threshold=0.0\n",
    ")\n",
    "\n",
    "print(\"\\nAgent Interaction Memory Results:\")\n",
    "print(\"-\" * 80)\n",
    "for result in interaction_results:\n",
    "    print(f\"Context: {result['context'][:200]}...\")  # Limit output size\n",
    "    print(f\"Score: {result['score']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
