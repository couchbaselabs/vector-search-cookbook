{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c60986a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this guide, we will walk you through building a powerful semantic search engine using Couchbase as the backend database and [Hugging Face](https://huggingface.co/) as the AI-powered embedding model provider. Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval.\n",
    "\n",
    "This tutorial demonstrates how to leverage Couchbase's **Global Secondary Index (GSI) vector search capabilities** with Hugging Face embeddings to create a high-performance semantic search system. GSI vector search in Couchbase offers significant advantages over traditional FTS (Full-Text Search) approaches, particularly for vector-first workloads and scenarios requiring complex filtering with high query-per-second (QPS) performance.\n",
    "\n",
    "This guide is designed to be comprehensive yet accessible, with clear step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system. Whether you're building a recommendation engine, content discovery platform, or any application requiring intelligent document retrieval, this tutorial provides the foundation you need.\n",
    "\n",
    "**Note**: If you want to perform semantic search using the FTS (Full-Text Search) index instead, please take a look at [this alternative approach](https://developer.couchbase.com//tutorial-huggingface-couchbase-vector-search-with-fts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178e6b3",
   "metadata": {},
   "source": [
    "# How to run this tutorial\n",
    "\n",
    "This tutorial is available as a Jupyter Notebook (`.ipynb` file) that you can run interactively. You can access the original notebook [here](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/huggingface/gsi/hugging_face.ipynb).\n",
    "\n",
    "You can either download the notebook file and run it on [Google Colab](https://colab.research.google.com/) or run it on your system by setting up the Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77308721",
   "metadata": {},
   "source": [
    "# Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208a54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain-couchbase==0.5.0rc1 transformers==4.56.1 sentence_transformers==5.1.0 langchain_huggingface==0.3.1 python-dotenv==1.1.1 ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9470f9e3-311b-45c8-81c3-baa5fe0995d2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd83070-32d7-4b22-9a7b-25b5c7e4d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "from langchain_huggingface.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.options import ClusterOptions\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_couchbase.cache import CouchbaseCache\n",
    "from langchain_couchbase.vectorstores import CouchbaseQueryVectorStore\n",
    "from langchain_couchbase.vectorstores import DistanceStrategy\n",
    "from langchain_couchbase.vectorstores import IndexType\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a3edf-f5f7-43e1-99b9-b775e94fbfe6",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "To run this tutorial successfully, you will need the following requirements:\n",
    "\n",
    "### Couchbase Requirements\n",
    "\n",
    "**Version Requirements:**\n",
    "- **Couchbase Server 8.0+** or **Couchbase Capella** with Query Service enabled\n",
    "- Note: GSI vector search is a newer feature that requires Couchbase Server 8.0 or above, unlike FTS-based vector search which works with 7.6.4+\n",
    "\n",
    "**Access Requirements:**\n",
    "- A configured Bucket, Scope, and Collection\n",
    "- User credentials with **Read and Write** access to your target collection\n",
    "- Network connectivity to your Couchbase cluster\n",
    "\n",
    "### Create and Deploy Your Free Tier Operational Cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy a forever free tier operational cluster. This account provides you with an environment where you can explore and learn about Capella with no time constraint.\n",
    "\n",
    "To learn more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
    "\n",
    "### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
    "\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the required bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running.\n",
    "\n",
    "### Python Environment Requirements\n",
    "\n",
    "- **Python 3.8+** \n",
    "- Required Python packages (installed via pip in the next section):\n",
    "  - `langchain-couchbase==0.5.0rc1`\n",
    "  - `transformers==4.56.1` \n",
    "  - `sentence_transformers==5.1.0`\n",
    "  - `langchain_huggingface==0.3.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56551dec-1029-4951-83f9-7899ee4cc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "# Configuration\n",
    "couchbase_cluster_url = os.getenv('CB_CLUSTER_URL') or input(\"Couchbase Cluster URL:\")\n",
    "couchbase_username = os.getenv('CB_USERNAME') or input(\"Couchbase Username:\")\n",
    "couchbase_password = os.getenv('CB_PASSWORD') or getpass.getpass(\"Couchbase password:\")\n",
    "couchbase_bucket = os.getenv('CB_BUCKET') or input(\"Couchbase Bucket:\")\n",
    "couchbase_scope = os.getenv('CB_SCOPE') or input(\"Couchbase Scope:\")\n",
    "couchbase_collection = os.getenv('CB_COLLECTION') or input(\"Couchbase Collection:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15edfec2-64bd-4ba1-b072-4fadacddb01a",
   "metadata": {},
   "source": [
    "# Couchbase Connection\n",
    "In this section, we first need to create a `PasswordAuthenticator` object that would hold our Couchbase credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae34ded-a52e-45bd-9712-c3bb5ea4f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = PasswordAuthenticator(\n",
    "    couchbase_username,\n",
    "    couchbase_password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b07864-1b17-4a6c-9127-3f74ab1117c3",
   "metadata": {},
   "source": [
    "Then, we use this object to connect to Couchbase Cluster and select specified above bucket, scope and collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea57ff8-2556-46d3-9211-3803420d93ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to cluster at URL: couchbase://localhost\n",
      "Connected to the cluster\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to cluster at URL: \" + couchbase_cluster_url)\n",
    "cluster = Cluster(couchbase_cluster_url, ClusterOptions(auth))\n",
    "cluster.wait_until_ready(timedelta(seconds=5))\n",
    "\n",
    "bucket = cluster.bucket(couchbase_bucket)\n",
    "scope = bucket.scope(couchbase_scope)\n",
    "collection = scope.collection(couchbase_collection)\n",
    "print(\"Connected to the cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625881d5-39e2-44ed-bbca-0db67e98f765",
   "metadata": {},
   "source": [
    "# Optimizing Vector Search with Global Secondary Index (GSI)\n",
    "\n",
    "With Couchbase 8.0+, you can leverage the power of GSI-based vector search, which offers significant performance improvements over traditional Full-Text Search (FTS) approaches for vector-first workloads. GSI vector search provides high-performance vector similarity search with advanced filtering capabilities and is designed to scale to billions of vectors.\n",
    "\n",
    "## GSI vs FTS: Choosing the Right Approach\n",
    "\n",
    "| Feature               | GSI Vector Search                                               | FTS Vector Search                         |\n",
    "| --------------------- | --------------------------------------------------------------- | ----------------------------------------- |\n",
    "| **Best For**          | Vector-first workloads, complex filtering, high QPS performance| Hybrid search and high recall rates      |\n",
    "| **Couchbase Version** | 8.0.0+                                                         | 7.6.4+                                    |\n",
    "| **Filtering**         | Pre-filtering with `WHERE` clauses (Composite) or post-filtering (BHIVE) | Pre-filtering with flexible ordering |\n",
    "| **Scalability**       | Up to billions of vectors (BHIVE)                              | Up to 10 million vectors                  |\n",
    "| **Performance**       | Optimized for concurrent operations with low memory footprint  | Good for mixed text and vector queries   |\n",
    "\n",
    "## GSI Vector Index Types\n",
    "\n",
    "Couchbase offers two distinct GSI vector index types, each optimized for different use cases:\n",
    "\n",
    "### Hyperscale Vector Indexes (BHIVE)\n",
    "\n",
    "- **Best for**: Pure vector searches like content discovery, recommendations, and semantic search\n",
    "- **Use when**: You primarily perform vector-only queries without complex scalar filtering\n",
    "- **Features**: \n",
    "  - High performance with low memory footprint\n",
    "  - Optimized for concurrent operations\n",
    "  - Designed to scale to billions of vectors\n",
    "  - Supports post-scan filtering for basic metadata filtering\n",
    "\n",
    "### Composite Vector Indexes\n",
    "\n",
    "- **Best for**: Filtered vector searches that combine vector similarity with scalar value filtering\n",
    "- **Use when**: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n",
    "- **Features**: \n",
    "  - Efficient pre-filtering where scalar attributes reduce the vector comparison scope\n",
    "  - Best for well-defined workloads requiring complex filtering using GSI features\n",
    "  - Supports range lookups combined with vector search\n",
    "\n",
    "## Why Choose GSI for This Tutorial?\n",
    "\n",
    "In this tutorial, we'll demonstrate creating a **BHIVE index** and running vector similarity queries using GSI. BHIVE is ideal for semantic search scenarios where you want:\n",
    "\n",
    "1. **High-performance vector search** across large datasets\n",
    "2. **Low latency** for real-time applications\n",
    "3. **Scalability** to handle growing vector collections\n",
    "4. **Concurrent operations** for multi-user environments\n",
    "\n",
    "The BHIVE index will provide optimal performance for our Hugging Face embedding-based semantic search implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a66753ba-22d4-4eaf-8275-ffd9fd53b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BHIVE GSI vector index (good default: IVF,SQ8)\n",
    "vector_store = CouchbaseQueryVectorStore(\n",
    "    cluster=cluster,\n",
    "    bucket_name=couchbase_bucket,\n",
    "    scope_name=couchbase_scope,\n",
    "    collection_name=couchbase_collection,\n",
    "    embedding=HuggingFaceEmbeddings(), # Hugging Face Initialization\n",
    "    distance_metric=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8e261-d670-4c40-8037-3d4e3084c360",
   "metadata": {},
   "source": [
    "# Embedding Documents\n",
    "\n",
    "Now that we have set up our vector store with Hugging Face embeddings, we can add documents to our collection. The `CouchbaseQueryVectorStore` automatically handles the embedding generation process using the Hugging Face transformers library.\n",
    "\n",
    "## Understanding the Embedding Process\n",
    "\n",
    "When we add text documents to our vector store, several important processes happen automatically:\n",
    "\n",
    "1. **Text Preprocessing**: The input text is preprocessed and tokenized according to the Hugging Face model's requirements\n",
    "2. **Vector Generation**: Each document is converted into a high-dimensional vector (embedding) that captures its semantic meaning\n",
    "3. **Storage**: The embeddings are stored in Couchbase along with the original text and any metadata\n",
    "4. **Indexing**: The vectors are indexed using our BHIVE GSI index for efficient similarity search\n",
    "\n",
    "## Adding Sample Documents\n",
    "\n",
    "In this example, we're adding sample documents that demonstrate Couchbase's capabilities. The system will:\n",
    "- Generate embeddings for each text document using the Hugging Face model\n",
    "- Store them in our Couchbase collection\n",
    "- Make them immediately available for semantic search once the GSI index is ready\n",
    "\n",
    "**Note**: The `batch_size` parameter controls how many documents are processed together, which can help optimize performance for large document sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe90414b-2611-4c99-9dfc-c0d634eb2af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c31ced04bcd74289acfcec58da1b5d02',\n",
       " '5c9eeae63a0f4ef39d2545fa8fc3f8e3',\n",
       " 'cde9928042294055b366a93a74754ed1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\",\n",
    "    \"It’s used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.\",\n",
    "    input(\"Enter custom embedding text:\")\n",
    "]\n",
    "vector_store.add_texts(texts=texts, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47117471",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = CouchbaseCache(\n",
    "    cluster=cluster,\n",
    "    bucket_name=couchbase_bucket,\n",
    "    scope_name=couchbase_scope,\n",
    "    collection_name=couchbase_collection,\n",
    ")\n",
    "set_llm_cache(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68ed11",
   "metadata": {},
   "source": [
    "# Understanding GSI Index Configuration (Couchbase 8.0 Feature)\n",
    "\n",
    "Before creating our BHIVE index, it's important to understand the configuration parameters that optimize vector storage and search performance. The `index_description` parameter controls how Couchbase optimizes vector storage through centroids and quantization.\n",
    "\n",
    "## Index Description Format: `'IVF[<centroids>],{PQ|SQ}<settings>'`\n",
    "\n",
    "### Centroids (IVF - Inverted File)\n",
    "- Controls how the dataset is subdivided for faster searches\n",
    "- **More centroids** = faster search, slower training time\n",
    "- **Fewer centroids** = slower search, faster training time\n",
    "- If omitted (like `IVF,SQ8`), Couchbase auto-selects based on dataset size\n",
    "\n",
    "### Quantization Options\n",
    "**Scalar Quantization (SQ):**\n",
    "- `SQ4`, `SQ6`, `SQ8` (4, 6, or 8 bits per dimension)\n",
    "- Lower memory usage, faster search, slightly reduced accuracy\n",
    "\n",
    "**Product Quantization (PQ):**\n",
    "- Format: `PQ<subquantizers>x<bits>` (e.g., `PQ32x8`)\n",
    "- Better compression for very large datasets\n",
    "- More complex but can maintain accuracy with smaller index size\n",
    "\n",
    "### Common Configuration Examples\n",
    "- **`IVF,SQ8`** - Auto centroids, 8-bit scalar quantization (good default)\n",
    "- **`IVF1000,SQ6`** - 1000 centroids, 6-bit scalar quantization\n",
    "- **`IVF,PQ32x8`** - Auto centroids, 32 subquantizers with 8 bits\n",
    "\n",
    "For detailed configuration options, see the [Couchbase Vector Index documentation](https://docs.couchbase.com/server/current/vector-index/hyperscale-vector-index.html#algo_settings).\n",
    "\n",
    "## Our Configuration Choice\n",
    "\n",
    "In this tutorial, we use `IVF,SQ8` which provides:\n",
    "- **Auto-selected centroids** optimized for our dataset size\n",
    "- **8-bit scalar quantization** for good balance of speed, memory usage, and accuracy\n",
    "- **COSINE distance metric** ideal for semantic similarity search\n",
    "- **Optimal performance** for most semantic search use cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BHIVE index\n",
    "vector_store.create_index(\n",
    "    index_type=IndexType.BHIVE,\n",
    "    index_description=\"IVF,SQ8\",\n",
    "    distance_metric=DistanceStrategy.COSINE,\n",
    "    index_name=\"huggingface_bhive_index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a0d98-bcf5-4fe4-b602-6e8a23edf95e",
   "metadata": {},
   "source": [
    "# Performing Semantic Search with GSI Vector Index\n",
    "\n",
    "Now that we have created our BHIVE GSI vector index and added documents to our collection, we can perform powerful semantic search queries. The `similarity_search_with_score` method allows us to find documents that are semantically similar to our query text.\n",
    "\n",
    "## How GSI Vector Search Works\n",
    "\n",
    "When you perform a search query with GSI vector search:\n",
    "\n",
    "1. **Query Embedding**: Your search text is converted into a vector embedding using the same Hugging Face model\n",
    "2. **Vector Similarity Calculation**: The GSI index efficiently compares your query vector against all stored document vectors\n",
    "3. **Distance Computation**: Using the COSINE distance metric, the system calculates similarity scores\n",
    "4. **Result Ranking**: Documents are ranked by their similarity scores and returned with their relevance scores\n",
    "5. **Post-processing**: Results include both the document content and metadata for further processing\n",
    "\n",
    "## Understanding Search Results\n",
    "\n",
    "The search results include:\n",
    "- **Document Content**: The original text that was embedded\n",
    "- **Similarity Score**: Lower scores indicate higher similarity (distance-based metric)\n",
    "- **Document ID**: Unique identifier for the document in Couchbase\n",
    "- **Metadata**: Any additional information stored with the document\n",
    "\n",
    "**Note**: In GSI vector search, the score represents the vector distance between the query and document embeddings. Lower distance values indicate higher similarity, which is the opposite of some other similarity systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e8283c-73ff-452e-98ee-e89fa992371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity search for phrase: \"name a multipurpose database with distributed capability\"\n",
      "[(Document(id='c31ced04bcd74289acfcec58da1b5d02', metadata={}, page_content='Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.'), 0.5401588405489548)]\n",
      "Found answer: c31ced04bcd74289acfcec58da1b5d02; score: 0.5401588405489548\n",
      "Answer text: Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\n",
      "------\n",
      "Vector similarity search for phrase: \"What was the data inside the sample text?\"\n",
      "[(Document(id='cde9928042294055b366a93a74754ed1', metadata={}, page_content='This is a sample text with the data \"Qwerty\"'), 0.5143860972617782)]\n",
      "Found answer: cde9928042294055b366a93a74754ed1; score: 0.5143860972617782\n",
      "Answer text: This is a sample text with the data \"Qwerty\"\n"
     ]
    }
   ],
   "source": [
    "def search_similar(text):\n",
    "    print(\"Vector similarity search for phrase: \\\"\" + text + \"\\\"\")\n",
    "    results = vector_store.similarity_search_with_score(text, k=1)\n",
    "    print(results)\n",
    "    for doc, score in results:\n",
    "        print(\"Found answer: \" + doc.id + \"; score: \" + str(score))\n",
    "        doc = collection.get(doc.id)\n",
    "        print(\"Answer text: \" + doc.value[\"text\"])\n",
    "        \n",
    "search_similar(\"name a multipurpose database with distributed capability\")\n",
    "print(\"------\")\n",
    "search_similar(input(\"Enter custom search phrase:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21290e6",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "You have successfully built a powerful semantic search engine using Couchbase's GSI vector search capabilities and Hugging Face embeddings. This guide has walked you through the complete process of creating a high-performance vector search system that can scale to handle billions of documents.\n",
    "\n",
    "## Next Steps and Extensions\n",
    "\n",
    "This foundation can be extended for more advanced use cases:\n",
    "\n",
    "1. **Add Metadata Filtering**: Implement complex filtering using Composite GSI indexes\n",
    "2. **Scale to Production**: Deploy with proper resource allocation and monitoring\n",
    "3. **Implement RAG Systems**: Build Retrieval-Augmented Generation applications\n",
    "4. **Add Real-time Updates**: Implement streaming document updates\n",
    "5. **Optimize for Specific Domains**: Fine-tune embeddings for your specific use case\n",
    "\n",
    "## Further Resources\n",
    "\n",
    "- [Couchbase Vector Search Documentation](https://docs.couchbase.com/cloud/vector-search/vector-search.html)\n",
    "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "- [LangChain Couchbase Integration](https://python.langchain.com/docs/integrations/vectorstores/couchbase)\n",
    "- [GSI Vector Index Configuration Guide](https://docs.couchbase.com/server/current/vector-index/hyperscale-vector-index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
