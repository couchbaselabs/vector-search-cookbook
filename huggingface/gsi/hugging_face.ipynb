{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6169179f",
   "metadata": {},
   "source": [
    "# Semantic Search with Couchbase GSI Vector Search and Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccb373",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e89bb2",
   "metadata": {},
   "source": [
    "In this guide, we will walk you through building a powerful semantic search engine using Couchbase as the backend database and [Hugging Face](https://huggingface.co/) as the AI-powered embedding model provider. Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval.\n",
    "\n",
    "This tutorial demonstrates how to leverage Couchbase's **Global Secondary Index (GSI) vector search capabilities** with Hugging Face embeddings to create a high-performance semantic search system. GSI vector search in Couchbase offers significant advantages over traditional FTS (Full-Text Search) approaches, particularly for vector-first workloads and scenarios requiring complex filtering with high query-per-second (QPS) performance.\n",
    "\n",
    "This guide is designed to be comprehensive yet accessible, with clear step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system. Whether you're building a recommendation engine, content discovery platform, or any application requiring intelligent document retrieval, this tutorial provides the foundation you need.\n",
    "\n",
    "**Note**: If you want to perform semantic search using the FTS (Full-Text Search) index instead, please take a look at [this alternative approach](https://developer.couchbase.com//tutorial-huggingface-couchbase-vector-search-with-fts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a6cef",
   "metadata": {},
   "source": [
    "## How to Run This Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675bac1",
   "metadata": {},
   "source": [
    "This tutorial is available as a Jupyter Notebook (`.ipynb` file) that you can run interactively. You can access the original notebook [here](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/huggingface/gsi/hugging_face.ipynb).\n",
    "\n",
    "You can either download the notebook file and run it on [Google Colab](https://colab.research.google.com/) or run it on your system by setting up the Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8131674b",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526738b",
   "metadata": {},
   "source": [
    "### Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8719a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain-couchbase==0.5.0rc1 transformers==4.56.1 sentence_transformers==5.1.0 langchain_huggingface==0.3.1 python-dotenv==1.1.1 ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688b5a5",
   "metadata": {},
   "source": [
    "### Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382fbd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "from langchain_huggingface.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.options import ClusterOptions\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_couchbase.cache import CouchbaseCache\n",
    "from langchain_couchbase.vectorstores import CouchbaseQueryVectorStore\n",
    "from langchain_couchbase.vectorstores import DistanceStrategy\n",
    "from langchain_couchbase.vectorstores import IndexType\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b0c46",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed6097",
   "metadata": {},
   "source": [
    "To run this tutorial successfully, you will need the following requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc4f41",
   "metadata": {},
   "source": [
    "#### Couchbase Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d2d46",
   "metadata": {},
   "source": [
    "**Version Requirements:**\n",
    "- **Couchbase Server 8.0+** or **Couchbase Capella** with Query Service enabled\n",
    "- Note: GSI vector search is a newer feature that requires Couchbase Server 8.0 or above, unlike FTS-based vector search which works with 7.6.4+\n",
    "\n",
    "**Access Requirements:**\n",
    "- A configured Bucket, Scope, and Collection\n",
    "- User credentials with **Read and Write** access to your target collection\n",
    "- Network connectivity to your Couchbase cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1004c27",
   "metadata": {},
   "source": [
    "#### Create and Deploy Your Free Tier Operational Cluster on Capella"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3fe0b",
   "metadata": {},
   "source": [
    "To get started with Couchbase Capella, create an account and use it to deploy a forever free tier operational cluster. This account provides you with an environment where you can explore and learn about Capella with no time constraint.\n",
    "\n",
    "To learn more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0150591",
   "metadata": {},
   "source": [
    "#### Couchbase Capella Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a846e2c",
   "metadata": {},
   "source": [
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
    "\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the required bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154749a7",
   "metadata": {},
   "source": [
    "#### Python Environment Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b4015",
   "metadata": {},
   "source": [
    "- **Python 3.8+** \n",
    "- Required Python packages (installed via pip in the next section):\n",
    "  - `langchain-couchbase==0.5.0rc1`\n",
    "  - `transformers==4.56.1` \n",
    "  - `sentence_transformers==5.1.0`\n",
    "  - `langchain_huggingface==0.3.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f839a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "# Configuration\n",
    "couchbase_cluster_url = os.getenv('CB_CLUSTER_URL') or input(\"Couchbase Cluster URL:\")\n",
    "couchbase_username = os.getenv('CB_USERNAME') or input(\"Couchbase Username:\")\n",
    "couchbase_password = os.getenv('CB_PASSWORD') or getpass.getpass(\"Couchbase password:\")\n",
    "couchbase_bucket = os.getenv('CB_BUCKET') or input(\"Couchbase Bucket:\")\n",
    "couchbase_scope = os.getenv('CB_SCOPE') or input(\"Couchbase Scope:\")\n",
    "couchbase_collection = os.getenv('CB_COLLECTION') or input(\"Couchbase Collection:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efcd2a",
   "metadata": {},
   "source": [
    "## Couchbase Connection Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92120f",
   "metadata": {},
   "source": [
    "### Create Authentication Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ecdf5b",
   "metadata": {},
   "source": [
    "In this section, we first need to create a `PasswordAuthenticator` object that would hold our Couchbase credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed08958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = PasswordAuthenticator(\n",
    "    couchbase_username,\n",
    "    couchbase_password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf198d8",
   "metadata": {},
   "source": [
    "### Connect to Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ae472",
   "metadata": {},
   "source": [
    "Then, we use this object to connect to Couchbase Cluster and select specified above bucket, scope and collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c1379eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to cluster at URL: couchbase://localhost\n",
      "Connected to the cluster\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to cluster at URL: \" + couchbase_cluster_url)\n",
    "cluster = Cluster(couchbase_cluster_url, ClusterOptions(auth))\n",
    "cluster.wait_until_ready(timedelta(seconds=5))\n",
    "\n",
    "bucket = cluster.bucket(couchbase_bucket)\n",
    "scope = bucket.scope(couchbase_scope)\n",
    "collection = scope.collection(couchbase_collection)\n",
    "print(\"Connected to the cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc060a",
   "metadata": {},
   "source": [
    "## Understanding GSI Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252a4e0",
   "metadata": {},
   "source": [
    "### Optimizing Vector Search with Global Secondary Index (GSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5be850",
   "metadata": {},
   "source": [
    "With Couchbase 8.0+, you can leverage the power of GSI-based vector search, which offers significant performance improvements over traditional Full-Text Search (FTS) approaches for vector-first workloads. GSI vector search provides high-performance vector similarity search with advanced filtering capabilities and is designed to scale to billions of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb71d0",
   "metadata": {},
   "source": [
    "#### GSI vs FTS: Choosing the Right Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab76b43",
   "metadata": {},
   "source": [
    "| Feature               | GSI Vector Search                                               | FTS Vector Search                         |\n",
    "| --------------------- | --------------------------------------------------------------- | ----------------------------------------- |\n",
    "| **Best For**          | Vector-first workloads, complex filtering, high QPS performance| Hybrid search and high recall rates      |\n",
    "| **Couchbase Version** | 8.0.0+                                                         | 7.6.4+                                    |\n",
    "| **Filtering**         | Pre-filtering with `WHERE` clauses (Composite) or post-filtering (BHIVE) | Pre-filtering with flexible ordering |\n",
    "| **Scalability**       | Up to billions of vectors (BHIVE)                              | Up to 10 million vectors                  |\n",
    "| **Performance**       | Optimized for concurrent operations with low memory footprint  | Good for mixed text and vector queries   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43a84c",
   "metadata": {},
   "source": [
    "#### GSI Vector Index Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b2ded",
   "metadata": {},
   "source": [
    "Couchbase offers two distinct GSI vector index types, each optimized for different use cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdfb94",
   "metadata": {},
   "source": [
    "##### Hyperscale Vector Indexes (BHIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4f265",
   "metadata": {},
   "source": [
    "- **Best for**: Pure vector searches like content discovery, recommendations, and semantic search\n",
    "- **Use when**: You primarily perform vector-only queries without complex scalar filtering\n",
    "- **Features**: \n",
    "  - High performance with low memory footprint\n",
    "  - Optimized for concurrent operations\n",
    "  - Designed to scale to billions of vectors\n",
    "  - Supports post-scan filtering for basic metadata filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a909f",
   "metadata": {},
   "source": [
    "##### Composite Vector Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da92c8",
   "metadata": {},
   "source": [
    "- **Best for**: Filtered vector searches that combine vector similarity with scalar value filtering\n",
    "- **Use when**: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n",
    "- **Features**: \n",
    "  - Efficient pre-filtering where scalar attributes reduce the vector comparison scope\n",
    "  - Best for well-defined workloads requiring complex filtering using GSI features\n",
    "  - Supports range lookups combined with vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22cdfe",
   "metadata": {},
   "source": [
    "#### Why Choose GSI for This Tutorial?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785206e8",
   "metadata": {},
   "source": [
    "In this tutorial, we'll demonstrate creating a **BHIVE index** and running vector similarity queries using GSI. BHIVE is ideal for semantic search scenarios where you want:\n",
    "\n",
    "1. **High-performance vector search** across large datasets\n",
    "2. **Low latency** for real-time applications\n",
    "3. **Scalability** to handle growing vector collections\n",
    "4. **Concurrent operations** for multi-user environments\n",
    "\n",
    "The BHIVE index will provide optimal performance for our Hugging Face embedding-based semantic search implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94d94f",
   "metadata": {},
   "source": [
    "#### Understanding GSI Index Configuration (Couchbase 8.0 Feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d5e66",
   "metadata": {},
   "source": [
    "Before creating our BHIVE index, it's important to understand the configuration parameters that optimize vector storage and search performance. The `index_description` parameter controls how Couchbase optimizes vector storage through centroids and quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e3b9f",
   "metadata": {},
   "source": [
    "##### Index Description Format: `'IVF[<centroids>],{PQ|SQ}<settings>'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff9ab8",
   "metadata": {},
   "source": [
    "###### Centroids (IVF - Inverted File)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5113f60",
   "metadata": {},
   "source": [
    "- Controls how the dataset is subdivided for faster searches\n",
    "- **More centroids** = faster search, slower training time\n",
    "- **Fewer centroids** = slower search, faster training time\n",
    "- If omitted (like `IVF,SQ8`), Couchbase auto-selects based on dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39802483",
   "metadata": {},
   "source": [
    "###### Quantization Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6debe",
   "metadata": {},
   "source": [
    "**Scalar Quantization (SQ):**\n",
    "- `SQ4`, `SQ6`, `SQ8` (4, 6, or 8 bits per dimension)\n",
    "- Lower memory usage, faster search, slightly reduced accuracy\n",
    "\n",
    "**Product Quantization (PQ):**\n",
    "- Format: `PQ<subquantizers>x<bits>` (e.g., `PQ32x8`)\n",
    "- Better compression for very large datasets\n",
    "- More complex but can maintain accuracy with smaller index size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e52c4",
   "metadata": {},
   "source": [
    "###### Common Configuration Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14739d04",
   "metadata": {},
   "source": [
    "- **`IVF,SQ8`** - Auto centroids, 8-bit scalar quantization (good default)\n",
    "- **`IVF1000,SQ6`** - 1000 centroids, 6-bit scalar quantization\n",
    "- **`IVF,PQ32x8`** - Auto centroids, 32 subquantizers with 8 bits\n",
    "\n",
    "For detailed configuration options, see the [Couchbase Vector Index documentation](https://docs.couchbase.com/server/current/vector-index/hyperscale-vector-index.html#algo_settings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdc18e",
   "metadata": {},
   "source": [
    "##### Our Configuration Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab293d1",
   "metadata": {},
   "source": [
    "In this tutorial, we use `IVF,SQ8` which provides:\n",
    "- **Auto-selected centroids** optimized for our dataset size\n",
    "- **8-bit scalar quantization** for good balance of speed, memory usage, and accuracy\n",
    "- **COSINE distance metric** ideal for semantic similarity search\n",
    "- **Optimal performance** for most semantic search use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fea6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BHIVE GSI vector index (good default: IVF,SQ8)\n",
    "vector_store = CouchbaseQueryVectorStore(\n",
    "    cluster=cluster,\n",
    "    bucket_name=couchbase_bucket,\n",
    "    scope_name=couchbase_scope,\n",
    "    collection_name=couchbase_collection,\n",
    "    embedding=HuggingFaceEmbeddings(), # Hugging Face Initialization\n",
    "    distance_metric=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32165ef0",
   "metadata": {},
   "source": [
    "## Document Processing and Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a3663",
   "metadata": {},
   "source": [
    "### Embedding Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032abad",
   "metadata": {},
   "source": [
    "Now that we have set up our vector store with Hugging Face embeddings, we can add documents to our collection. The `CouchbaseQueryVectorStore` automatically handles the embedding generation process using the Hugging Face transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854844f1",
   "metadata": {},
   "source": [
    "#### Understanding the Embedding Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dbfd35",
   "metadata": {},
   "source": [
    "When we add text documents to our vector store, several important processes happen automatically:\n",
    "\n",
    "1. **Text Preprocessing**: The input text is preprocessed and tokenized according to the Hugging Face model's requirements\n",
    "2. **Vector Generation**: Each document is converted into a high-dimensional vector (embedding) that captures its semantic meaning\n",
    "3. **Storage**: The embeddings are stored in Couchbase along with the original text and any metadata\n",
    "4. **Indexing**: The vectors are indexed using our BHIVE GSI index for efficient similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344575c6",
   "metadata": {},
   "source": [
    "#### Adding Sample Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b2bbe9",
   "metadata": {},
   "source": [
    "In this example, we're adding sample documents that demonstrate Couchbase's capabilities. The system will:\n",
    "- Generate embeddings for each text document using the Hugging Face model\n",
    "- Store them in our Couchbase collection\n",
    "- Make them immediately available for semantic search once the GSI index is ready\n",
    "\n",
    "**Note**: The `batch_size` parameter controls how many documents are processed together, which can help optimize performance for large document sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff1abb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c94df7ea72984e1db2b4e56dda4f3364',\n",
       " '7c28068f849e4bf596345b4cf982eb7f',\n",
       " 'c10af04b30df49f88f5f92512f0e6a3e']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\",\n",
    "    \"It’s used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.\",\n",
    "    input(\"Enter custom embedding text:\")\n",
    "]\n",
    "vector_store.add_texts(texts=texts, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47b607",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6419885",
   "metadata": {},
   "source": [
    "Now let's demonstrate how to build a complete RAG system by comparing different optimization approaches. We'll show the performance progression with three distinct stages:\n",
    "\n",
    "1. **Raw Vector Search**: Basic vector similarity search without any optimizations\n",
    "2. **Cached Vector Search**: Same search with LLM response caching enabled to show cache benefits\n",
    "3. **GSI-Optimized RAG**: High-performance search using BHIVE GSI index with a different query for fair testing\n",
    "\n",
    "This progression demonstrates how each optimization contributes to building a production-ready RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63935fed",
   "metadata": {},
   "source": [
    "### Understanding Vector Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd6c58c",
   "metadata": {},
   "source": [
    "Before we start our RAG comparisons, let's understand what the search results mean:\n",
    "\n",
    "When you perform a search query with vector search:\n",
    "\n",
    "1. **Query Embedding**: Your search text is converted into a vector embedding using the Hugging Face model\n",
    "2. **Vector Similarity Calculation**: The system compares your query vector against all stored document vectors\n",
    "3. **Distance Computation**: Using the COSINE distance metric, the system calculates similarity distances\n",
    "4. **Result Ranking**: Documents are ranked by their distance values (lower = more similar)\n",
    "5. **Post-processing**: Results include both the document content and metadata\n",
    "\n",
    "**Note**: The returned value represents the vector distance between query and document embeddings. Lower distance values indicate higher similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcc939",
   "metadata": {},
   "source": [
    "### RAG Search Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833a536",
   "metadata": {},
   "source": [
    "Let's create a comprehensive search function for our RAG performance comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529df9bd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def rag_search_with_metrics(query_text, stage_name):\n",
    "    \"\"\"Perform semantic search with detailed performance metrics for RAG comparison\"\"\"\n",
    "    print(f\"\\n=== {stage_name.upper()} ===\")\n",
    "    print(f\"Query: \\\"{query_text}\\\"\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = vector_store.similarity_search_with_score(query_text, k=1)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    search_time = end_time - start_time\n",
    "    print(f\"Search Time: {search_time:.4f} seconds\")\n",
    "    \n",
    "    for doc, distance in results:\n",
    "        print(f\"Document ID: {doc.id}\")\n",
    "        print(f\"Vector Distance: {distance:.6f} (lower = more similar)\")\n",
    "        \n",
    "        # Retrieve full document\n",
    "        full_doc = collection.get(doc.id)\n",
    "        print(f\"Retrieved Text: {full_doc.value['text']}\")\n",
    "        \n",
    "    return search_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4ada9",
   "metadata": {},
   "source": [
    "### Stage 1: Raw Vector Search (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e6110",
   "metadata": {},
   "source": [
    "First, let's establish baseline performance with raw vector search - no optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c994c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 1: RAW VECTOR SEARCH ===\n",
      "Query: \"What are the key features of a scalable NoSQL database?\"\n",
      "Search Time: 0.1136 seconds\n",
      "Document ID: c94df7ea72984e1db2b4e56dda4f3364\n",
      "Vector Distance: 0.586197 (lower = more similar)\n",
      "Retrieved Text: Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What are the key features of a scalable NoSQL database?\"\n",
    "raw_time = rag_search_with_metrics(test_query, \"Stage 1: Raw Vector Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430eea16",
   "metadata": {},
   "source": [
    "### Stage 2: Enable Couchbase Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b6643",
   "metadata": {},
   "source": [
    "Now let's enable Couchbase caching and test with the SAME query to see cache benefits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a8dc9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couchbase LLM cache enabled for improved RAG performance!\n",
      "\n",
      "=== STAGE 2: WITH CACHE (SAME QUERY) ===\n",
      "Query: \"What are the key features of a scalable NoSQL database?\"\n",
      "Search Time: 0.0652 seconds\n",
      "Document ID: c94df7ea72984e1db2b4e56dda4f3364\n",
      "Vector Distance: 0.586197 (lower = more similar)\n",
      "Retrieved Text: Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\n"
     ]
    }
   ],
   "source": [
    "# Set up Couchbase cache for LLM responses\n",
    "cache = CouchbaseCache(\n",
    "    cluster=cluster,\n",
    "    bucket_name=couchbase_bucket,\n",
    "    scope_name=couchbase_scope,\n",
    "    collection_name=couchbase_collection,\n",
    ")\n",
    "set_llm_cache(cache)\n",
    "\n",
    "print(\"Couchbase LLM cache enabled for improved RAG performance!\")\n",
    "\n",
    "# Test the SAME query with cache enabled to show cache benefits\n",
    "cached_time = rag_search_with_metrics(test_query, \"Stage 2: With Cache (Same Query)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23c42b",
   "metadata": {},
   "source": [
    "### Stage 3: Create BHIVE GSI Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629304c3",
   "metadata": {},
   "source": [
    "Now let's create the BHIVE GSI index using the configuration we discussed earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd43ce27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BHIVE GSI vector index...\n",
      "BHIVE GSI vector index created successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create BHIVE index\n",
    "print(\"Creating BHIVE GSI vector index...\")\n",
    "vector_store.create_index(\n",
    "    index_type=IndexType.BHIVE,\n",
    "    index_description=\"IVF,SQ8\",\n",
    "    distance_metric=DistanceStrategy.COSINE,\n",
    "    index_name=\"huggingface_bhive_index\",\n",
    ")\n",
    "print(\"BHIVE GSI vector index created successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a23d00",
   "metadata": {},
   "source": [
    "### Stage 3: GSI-Optimized RAG with Different Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5384e35",
   "metadata": {},
   "source": [
    "Now let's test with a DIFFERENT query to ensure fair comparison (avoiding cached results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54033ffb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 3: GSI-OPTIMIZED RAG (DIFFERENT QUERY) ===\n",
      "Query: \"How does a distributed database handle high-speed operations?\"\n",
      "Search Time: 0.0895 seconds\n",
      "Document ID: c94df7ea72984e1db2b4e56dda4f3364\n",
      "Vector Distance: 0.632770 (lower = more similar)\n",
      "Retrieved Text: Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\n"
     ]
    }
   ],
   "source": [
    "# Use a different query for fair testing (not cached)\n",
    "different_query = \"How does a distributed database handle high-speed operations?\"\n",
    "gsi_time = rag_search_with_metrics(different_query, \"Stage 3: GSI-Optimized RAG (Different Query)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e87bd",
   "metadata": {},
   "source": [
    "### Complete RAG Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b88363",
   "metadata": {},
   "source": [
    "Let's analyze the complete performance progression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1e76461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPLETE RAG PERFORMANCE ANALYSIS\n",
      "======================================================================\n",
      "Stage 1 - Raw Vector Search:           0.1136 seconds\n",
      "Stage 2 - With Cache (same query):     0.0652 seconds\n",
      "Stage 3 - GSI Index (different query): 0.0895 seconds\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "OPTIMIZATION BENEFITS:\n",
      "----------------------------------------------------------------------\n",
      "Cache Optimization:     1.74x faster (42.5% improvement)\n",
      "GSI Index Optimization: 1.27x faster (21.2% improvement)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE RAG PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Stage 1 - Raw Vector Search:           {raw_time:.4f} seconds\")\n",
    "print(f\"Stage 2 - With Cache (same query):     {cached_time:.4f} seconds\")\n",
    "print(f\"Stage 3 - GSI Index (different query): {gsi_time:.4f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"OPTIMIZATION BENEFITS:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Cache improvement\n",
    "if cached_time < raw_time:\n",
    "    cache_improvement = raw_time / cached_time\n",
    "    cache_saved = raw_time - cached_time\n",
    "    cache_percent = (cache_saved / raw_time) * 100\n",
    "    print(f\"Cache Optimization:     {cache_improvement:.2f}x faster ({cache_percent:.1f}% improvement)\")\n",
    "else:\n",
    "    print(f\"Cache Optimization:     No significant improvement (first-time query)\")\n",
    "\n",
    "# GSI improvement (comparing to raw time for context)\n",
    "if gsi_time < raw_time:\n",
    "    gsi_improvement = raw_time / gsi_time\n",
    "    gsi_saved = raw_time - gsi_time\n",
    "    gsi_percent = (gsi_saved / raw_time) * 100\n",
    "    print(f\"GSI Index Optimization: {gsi_improvement:.2f}x faster ({gsi_percent:.1f}% improvement)\")\n",
    "else:\n",
    "    print(f\"GSI Index Optimization: Performance similar to baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c33c4",
   "metadata": {},
   "source": [
    "### Interactive RAG Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e6a46",
   "metadata": {},
   "source": [
    "Try your own queries to see the optimized RAG system in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2227ec26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INTERACTIVE GSI-OPTIMIZED RAG ===\n",
      "Query: \"what is the custom text with the data?\"\n",
      "Search Time: 0.1133 seconds\n",
      "Document ID: c10af04b30df49f88f5f92512f0e6a3e\n",
      "Vector Distance: 0.416787 (lower = more similar)\n",
      "Retrieved Text: this is a sample text with the data \"how\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11325693130493164"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_query = input(\"Enter your RAG query: \")\n",
    "rag_search_with_metrics(custom_query, \"Interactive GSI-Optimized RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0ace6",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414eb023",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [
    "You have successfully built a powerful semantic search engine using Couchbase's GSI vector search capabilities and Hugging Face embeddings. This guide has walked you through the complete process of creating a high-performance vector search system that can scale to handle billions of documents."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
